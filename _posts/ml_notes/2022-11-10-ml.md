---
title: ML compendium
blog_type: ml_notes
excerpt: Summary of many things ML.
layout: post_with_toc
---
## Entropy & KL-Divergence
Credits: [stats.stackexchange](https://stats.stackexchange.com/a/357974/84357)

> Below $$ \log = \ln $$

### Entropy
Measures the uncertainty in a system. In an information theoretic view, it's the
amount of information needed to remove the uncertainty in the system.

$$Entropy(x) = S(x) = - \sum_{x \in X}P(x)\log P(x) = - E_{x \sim X} [\log P(x)]$$

Above we are sampling $$x$$, from the distribution $$X$$: that is the probability of
sampling the specific value $$x$$ is defined by the probability distribution $$X$$.
Please see the `stats.stackexchange` link above for examples on Entropy.

Here's a [jupyter notebook](https://nbviewer.org/gist/psvishnu91/738cbc59e9f80fa72c3942e9aa2cfd48) 
with visualisation of entropy as a function of how spread the distribution is.

<iframe 
    src="https://nbviewer.org/gist/psvishnu91/738cbc59e9f80fa72c3942e9aa2cfd48"
    title="Entropy as a function of how spread out a distribution is."
    width="100%" height="400"
>
</iframe>

### KL-Divergence
Read the treatise on KL-D from [UIUC](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf).

**Discrete case**

$$D_{KL}(p(x)||q(x)) =  \sum_{x \in X} P(x) \log p(x) - P(x) \log q(x) =  \sum_{x \in X} P(x) \log \frac{p(x)}{q(x)}$$

## Loss function for models
1. Binary or Multinomial Logistic regression

The loss function is 